{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers==0.3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from STSDataReaderBinaryPositives import STSDataReaderBinaryPositives\n",
    "from MNRLoss import MNRLoss\n",
    "from MNRShuffler import ShuffledSentencesDataset, ShuffledSentenceTransformer, ModelExampleBasedShuffler\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import os\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, evaluation\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "import random\n",
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import pyltr\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers.util import pytorch_cos_sim\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for el in tqdm(os.listdir('datasets/CLEF/train/vclaims')):\n",
    "    res.append(pd.read_json(os.path.join('datasets/CLEF/train/vclaims', el), typ='series'))\n",
    "res = pd.concat(res,axis=1).T\n",
    "res = res.assign(joined=lambda x: x['vclaim'] + ' [SEP] ' + x['title'])\n",
    "res = res.assign(joined_sub=lambda x: x['vclaim'] + ' [SEP] ' + x['subtitle'])\n",
    "res = res.assign(joined_all=lambda x: x['vclaim'] + ' [SEP] ' + x['title'] + ' [SEP] ' + x['subtitle'])\n",
    "res.to_csv('datasets/CLEF/train/vclaims.tsv', sep='\\t', index=False)\n",
    "\n",
    "res = []\n",
    "for el in tqdm(os.listdir('datasets/CLEF/semi-supervised-data-clef-format/vclaims')):\n",
    "    res.append(pd.read_json(os.path.join('datasets/CLEF/semi-supervised-data-clef-format/vclaims', el),\n",
    "                            typ='series'))\n",
    "res = pd.concat(res,axis=1).T\n",
    "res = res.assign(joined=lambda x: x['vclaim'] + ' [SEP] ' + x['title'])\n",
    "res = res.assign(joined_sub=lambda x: x['vclaim'] + ' [SEP] ' + x['subtitle'])\n",
    "res = res.assign(joined_all=lambda x: x['vclaim'] + ' [SEP] ' + x['title'] + ' [SEP] ' + x['subtitle'])\n",
    "res.to_csv('datasets/CLEF/train/new_vclaims.tsv', sep='\\t', index=False)\n",
    "\n",
    "res = pd.concat([pd.read_csv('datasets/CLEF/train/vclaims.tsv', sep='\\t'), pd.read_csv('datasets/CLEF/train/new_vclaims.tsv', sep='\\t')])\n",
    "res.reset_index(drop=True, inplace=True)\n",
    "res.to_csv('datasets/CLEF/train/new_vclaims.tsv', sep='\\t', index=False)\n",
    "\n",
    "#____________________________________________________________________________________________\n",
    "\n",
    "\n",
    "verclaims = pd.read_csv('datasets/CLEF/train/new_vclaims.tsv', sep='\\t')\n",
    "res = pd.concat([\n",
    "    pd.read_csv('datasets/CLEF/train/tweets-train-dev.tsv', sep='\\t', header=None),\n",
    "    pd.read_csv('datasets/CLEF/semi-supervised-data-clef-format/tweets-all.tsv', sep='\\t', header=None)\n",
    "])\n",
    "res.reset_index(drop=True, inplace=True)\n",
    "res.to_csv('datasets/CLEF/train/new_tweets-train-dev.tsv', sep='\\t', index=False)\n",
    "\n",
    "tweets = pd.read_csv('datasets/CLEF/train/new_tweets-train-dev.tsv', sep='\\t')\n",
    "tweets.columns = ['iclaim_id', 'iclaim']\n",
    "\n",
    "res = pd.concat([\n",
    "    pd.read_csv('datasets/CLEF/train/qrels-train.tsv', sep='\\t', header=None),\n",
    "    pd.read_csv('datasets/CLEF/semi-supervised-data-clef-format/qrels-train-30.tsv',  sep='\\t', header=None)\n",
    "])\n",
    "res.reset_index(drop=True, inplace=True)\n",
    "res.to_csv('datasets/CLEF/train/new_qrels-train.tsv', sep='\\t', index=False)\n",
    "\n",
    "qrels = pd.read_csv('datasets/CLEF/train/new_qrels-train.tsv', sep='\\t')\n",
    "qrels.columns = ['iclaim_id', 'pad', 'vclaim_id', 'relevance']\n",
    "qrels = qrels.join(tweets.set_index('iclaim_id'), on='iclaim_id')\n",
    "qrels = qrels.join(verclaims.set_index('vclaim_id'), on='vclaim_id')\n",
    "qrels.to_csv('datasets/CLEF/train/semi_train_part.tsv', sep='\\t', index=False)\n",
    "pd.concat([qrels.iloc[:800], qrels.iloc[999:]\n",
    "          ]).to_csv('datasets/CLEF/train/train_part.tsv', sep='\\t', index=False)\n",
    "qrels.iloc[800:999].to_csv('datasets/CLEF/train/dev_part.tsv', sep='\\t', index=False)\n",
    "\n",
    "qrels = pd.read_csv('datasets/CLEF/train/qrels-dev.tsv', sep='\\t', header=None)\n",
    "qrels.columns = ['iclaim_id', 'pad', 'vclaim_id', 'relevance']\n",
    "qrels = qrels.join(tweets.set_index('iclaim_id'), on='iclaim_id')\n",
    "qrels.to_csv('datasets/CLEF/train/semi_dev_part.tsv', sep='\\t', index=False)\n",
    "qrels.to_csv('datasets/CLEF/train/test.tsv', sep='\\t', index=False)\n",
    "\n",
    "tweets_test = pd.read_csv('datasets/CLEF/test/subtask-2a--english/tweets-test.tsv', sep='\\t').drop_duplicates(subset=['iclaim_id'])\n",
    "tweets_test.columns = ['iclaim_id', 'iclaim']\n",
    "qrels_test = pd.read_csv('datasets/CLEF/predictions/qrels-test.tsv', sep='\\t', header=None)\n",
    "qrels_test.columns = ['iclaim_id', 'pad', 'vclaim_id', 'relevance']\n",
    "qrels_test = qrels_test.join(tweets_test.set_index('iclaim_id'), on='iclaim_id')\n",
    "qrels_test.to_csv('datasets/CLEF/train/semi_test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for seed in [4]:\n",
    "    for start in [2]:\n",
    "        for alpha_thr in [0.9]:\n",
    "            model = ShuffledSentenceTransformer('stsb-bert-base')\n",
    "\n",
    "            sts_reader_pos = STSDataReaderBinaryPositives('datasets/CLEF/', \n",
    "                    s1_col_idx=4, s2_col_idx=12, score_col_idx=3,normalize_scores=False, thr=0.6, get_positives=False)\n",
    "\n",
    "            train_batch_size = 8\n",
    "            num_epochs = 8\n",
    "\n",
    "            train_data_MNR = ShuffledSentencesDataset(sts_reader_pos.get_examples('train/semi_train_part.tsv'), model\n",
    "                                                     , ref_alpha=alpha_thr, start_ref_epoch=start)\n",
    "            train_dataloader_MNR = DataLoader(train_data_MNR, shuffle=False, batch_size=train_batch_size)\n",
    "            use_rescale = False\n",
    "            if alpha_thr != 1:\n",
    "                use_rescale = True\n",
    "            train_loss_MNR = MNRLoss(model=model, norm_dim=1, tau=None, use_rescale=use_rescale)\n",
    "\n",
    "            max_corpus_size = 30000\n",
    "\n",
    "            ir_queries = {}\n",
    "            ir_needed_qids = set()\n",
    "            ir_corpus = {}\n",
    "            ir_relevant_docs = {}\n",
    "\n",
    "            with open(os.path.join('datasets/CLEF/', 'train/semi_dev_part.tsv'), encoding='utf8') as fIn:\n",
    "                next(fIn)\n",
    "                for line in fIn:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    qid, query, duplicate_ids = line[0], line[4], line[2].split(',')\n",
    "                    ir_queries[qid] = query\n",
    "                    ir_relevant_docs[qid] = set(duplicate_ids)\n",
    "\n",
    "                    for qid in duplicate_ids:\n",
    "                        ir_needed_qids.add(qid)\n",
    "            distraction_questions = {}\n",
    "            verclaims = pd.read_csv('datasets/CLEF/train/vclaims.tsv', sep='\\t')\n",
    "            for i, line in verclaims.iterrows():\n",
    "                qid, question = str(line['vclaim_id']), str(line['joined_all'])\n",
    "\n",
    "                if qid in ir_needed_qids:\n",
    "                    ir_corpus[qid] = question\n",
    "                else:\n",
    "                    distraction_questions[qid] = question\n",
    "\n",
    "            other_qid_list = list(distraction_questions.keys())\n",
    "            random.shuffle(other_qid_list)\n",
    "\n",
    "            for qid in other_qid_list[0:max(0, max_corpus_size-len(ir_corpus))]:\n",
    "                ir_corpus[qid] = distraction_questions[qid]\n",
    "\n",
    "            ir_evaluator = evaluation.InformationRetrievalEvaluator(ir_queries, ir_corpus, ir_relevant_docs,\n",
    "                                                            map_at_k=[1,3,5,10,2000], mrr_at_k=[1,3], accuracy_at_k=[1])\n",
    "            max_corpus_size = 30000\n",
    "\n",
    "            ir_queries = {}\n",
    "            ir_needed_qids = set()\n",
    "            ir_corpus = {}\n",
    "            ir_relevant_docs = {}\n",
    "\n",
    "            with open(os.path.join('datasets/CLEF/', 'train/semi_test.tsv'), encoding='utf8') as fIn:\n",
    "                next(fIn)\n",
    "                for line in fIn:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    qid, query, duplicate_ids = line[0], line[4], line[2].split(',')\n",
    "                    ir_queries[qid] = query\n",
    "                    ir_relevant_docs[qid] = set(duplicate_ids)\n",
    "\n",
    "                    for qid in duplicate_ids:\n",
    "                        ir_needed_qids.add(qid)\n",
    "\n",
    "\n",
    "            distraction_questions = {}\n",
    "            verclaims = pd.read_csv('datasets/CLEF/train/vclaims.tsv', sep='\\t')\n",
    "            for i, line in verclaims.iterrows():\n",
    "                qid, question = str(line['vclaim_id']), str(line['joined_all'])\n",
    "                if qid in ir_needed_qids:\n",
    "                    ir_corpus[qid] = question\n",
    "                else:\n",
    "                    distraction_questions[qid] = question\n",
    "\n",
    "            other_qid_list = list(distraction_questions.keys())\n",
    "            random.shuffle(other_qid_list)\n",
    "\n",
    "            for qid in other_qid_list[0:max(0, max_corpus_size-len(ir_corpus))]:\n",
    "                ir_corpus[qid] = distraction_questions[qid]\n",
    "\n",
    "            ir_evaluator_test = evaluation.InformationRetrievalEvaluator(ir_queries, ir_corpus, ir_relevant_docs,\n",
    "                                                            map_at_k=[1,3,5,10,2000], mrr_at_k=[1,3], accuracy_at_k=[1])\n",
    "\n",
    "            max_corpus_size = 30000\n",
    "\n",
    "            ir_queries = {}\n",
    "            ir_needed_qids = set()\n",
    "            ir_corpus = {}\n",
    "            ir_relevant_docs = {}\n",
    "            \n",
    "            seq_evaluator = evaluation.SequentialEvaluator([ir_evaluator, ir_evaluator_test],\n",
    "                                                           main_score_function=lambda scores: scores[0])\n",
    "\n",
    "            shuffler = ModelExampleBasedShuffler(group_size=4, allow_same=True)\n",
    "            warmup_steps = math.ceil(len(train_data_MNR)*num_epochs/train_batch_size*0.1)\n",
    "            model_save_path = 'checkpoints_/model_1e-5_6_{}_{}_{}'.format(alpha_thr, start, seed)\n",
    "            shutil.rmtree(model_save_path, ignore_errors=True)\n",
    "\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "            model.fit(train_objectives=[(train_dataloader_MNR, train_loss_MNR)],\n",
    "                      evaluator=seq_evaluator,\n",
    "                      epochs=num_epochs,\n",
    "                      evaluation_steps=1000,\n",
    "                      warmup_steps=warmup_steps,\n",
    "                      output_path=model_save_path,\n",
    "                      optimizer_params={'alpha_lr':0.4, 'lr': 1e-5},\n",
    "                      shuffler=shuffler,\n",
    "                      shuffle_idxs=[0]\n",
    "                     )\n",
    "\n",
    "            model = ShuffledSentenceTransformer(model_save_path)\n",
    "\n",
    "            ir_evaluator_test.csv_file = \"1e-5_6_{}_{}_{}_test.csv\".format(alpha_thr, start, seed)\n",
    "            ir_evaluator.csv_file = \"1e-5_6_{}_{}_{}.csv\".format(alpha_thr, start, seed)\n",
    "            ir_evaluator_test(model, output_path='evaluations/')\n",
    "            ir_evaluator(model, output_path='evaluations/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TFIDF:\n",
    "    def __init__(self, vectorizer):\n",
    "        self.model = vectorizer\n",
    "    \n",
    "    def encode(self, query, show_progress_bar, batch_size, convert_to_tensor):\n",
    "        return torch.Tensor(self.model.transform(query).todense())\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True)\n",
    "vectorizer.fit(ir_evaluator.corpus)  # repeat with different data (e. g. with line['joined'])\n",
    "model = TFIDF(vectorizer)\n",
    "ir_evaluator(model)\n",
    "\n",
    "with open('checkpoints_/tfidf.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verclaims = pd.read_csv('datasets/CLEF/train/vclaims.tsv', sep='\\t')\n",
    "queries = pd.read_csv('datasets/CLEF/train/dev_part.tsv', sep='\\t')\n",
    "queries_test = pd.read_csv('datasets/CLEF/train/test.tsv', sep='\\t')\n",
    "qrels_test = queries_test[['iclaim_id', 'iclaim']].drop_duplicates(subset=['iclaim_id'])\n",
    "tweets_test = pd.read_csv('datasets/CLEF/test/subtask-2a--english/tweets-test.tsv', sep='\\t').drop_duplicates(subset=['iclaim_id'])\n",
    "\n",
    "with open('checkpoints_/tfidf.pkl', 'rb') as f:\n",
    "    tf_idf = pickle.load(f)\n",
    "    corpus = verclaims['vclaim'].values\n",
    "    tf_idf_embeds = tf_idf.transform(corpus)\n",
    "\n",
    "with open('checkpoints_/tfidf_joined.pkl', 'rb') as f:\n",
    "    tf_idf_joined = pickle.load(f)\n",
    "    corpus_joined = verclaims['joined'].values\n",
    "    tf_idf_joined_embeds = tf_idf_joined.transform(corpus_joined)\n",
    "    \n",
    "with open('checkpoints_/tfidf_joined_all.pkl', 'rb') as f:\n",
    "    tf_idf_joined_all = pickle.load(f)\n",
    "    corpus_joined_all = verclaims['joined_all'].values.astype(str)\n",
    "    tf_idf_joined_all_embeds = tf_idf_joined_all.transform(corpus_joined_all)\n",
    "    \n",
    "embedder_base = SentenceTransformer('model1')\n",
    "embeds_base_joined = embedder_base.encode(corpus_joined)\n",
    "embedder_large_joined = SentenceTransformer('model2')\n",
    "embeds_large_joined = embedder_large_joined.encode(corpus_joined)\n",
    "embedder_large_joined_all = SentenceTransformer('model3')\n",
    "embeds_large_joined_all = embedder_large_joined_all.encode(corpus_joined_all)\n",
    "\n",
    "\n",
    "def format_str(rel, qid, features, doc_id, query_id):\n",
    "    features_str = []\n",
    "    for i, el in enumerate(features):\n",
    "        features_str.append(str(i + 1) + ':' + str(el))\n",
    "    return '{} qid:{} '.format(rel, qid) + \" \".join(features_str) + ' #' + doc_id + '|' + query_id\n",
    "\n",
    "\n",
    "def get_features(queries, file, max_wrote=30, train_mode=True):\n",
    "    if train_mode:\n",
    "        gold_ids = queries['vclaim_id'].values\n",
    "    queries_ids = queries['iclaim_id'].values\n",
    "    queries = queries['iclaim'].values\n",
    "    embeds_base_queries = embedder_base.encode(queries)\n",
    "    embeds_large_joined_all_queries = embedder_large_joined_all.encode(queries)\n",
    "    embeds_large_joined_queries = embedder_large_joined.encode(queries)\n",
    "\n",
    "    data = []\n",
    "    with open(file, 'w') as f:\n",
    "        for qid, query in tqdm(enumerate(queries)):\n",
    "            scores_embeds_large_joined = pytorch_cos_sim([embeds_large_joined_queries[qid]],\n",
    "                                                               embeds_large_joined)[0].numpy()\n",
    "            scores_embeds_large_joined_all = pytorch_cos_sim([embeds_large_joined_all_queries[qid]],\n",
    "                                                               embeds_large_joined_all)[0].numpy()\n",
    "            scores_embeds_base = pytorch_cos_sim([embeds_base_queries[qid]], \n",
    "                                                          embeds_base_joined)[0].numpy()\n",
    "            scores_joined_all = cosine_similarity(tf_idf_joined_all.transform([query]), tf_idf_joined_all_embeds)[0]\n",
    "            scores_joined = cosine_similarity(tf_idf_joined.transform([query]), tf_idf_joined_embeds)[0]\n",
    "            scores = cosine_similarity(tf_idf.transform([query]), tf_idf_embeds)[0]\n",
    "            \n",
    "            repr_rank_embeds_large_joined = np.argsort(scores_embeds_large_joined)[::-1].argsort()\n",
    "            repr_rank_embeds_large_joined_all = np.argsort(scores_embeds_large_joined_all)[::-1].argsort()\n",
    "            repr_rank_embeds_base = np.argsort(scores_embeds_base)[::-1].argsort()\n",
    "            repr_rank_joined_all = np.argsort(scores_joined_all)[::-1].argsort()\n",
    "            repr_rank_joined = np.argsort(scores_joined)[::-1].argsort()\n",
    "            repr_rank = np.argsort(scores)[::-1].argsort()\n",
    "            \n",
    "            features = np.stack([\n",
    "                scores_embeds_large_joined, repr_rank_embeds_large_joined,\n",
    "                scores_embeds_large_joined_all, repr_rank_embeds_large_joined_all,\n",
    "                scores_embeds_base, repr_rank_embeds_base,\n",
    "                scores_joined_all, repr_rank_joined_all,\n",
    "                scores_joined, repr_rank_joined,\n",
    "                scores, repr_rank,\n",
    "            ], axis=1)\n",
    "            \n",
    "            features_dict = dict(zip(verclaims['vclaim_id'].values, features))\n",
    "            \n",
    "            if train_mode:\n",
    "                gold_id = gold_ids[qid]\n",
    "                f.write(format_str('1', qid, features_dict[gold_id], gold_id, queries_ids[qid]))\n",
    "                f.write('\\n')\n",
    "            \n",
    "            top_hits = sorted(features_dict.items(), key=lambda item: item[1][1])\n",
    "            wrote = 0\n",
    "            for el in top_hits:\n",
    "                if wrote >= max_wrote:\n",
    "                    break\n",
    "                if not train_mode or el[0] != gold_id:\n",
    "                    f.write(format_str('0', qid, el[1], el[0], queries_ids[qid]))\n",
    "                    f.write('\\n')\n",
    "                    wrote += 1\n",
    "                    \n",
    "get_features(queries, 'datasets/CLEF/train/new_reranking_dev_part.txt', max_wrote=100)\n",
    "get_features(queries_test, 'datasets/CLEF/train/new_reranking_test.txt', max_wrote=100)\n",
    "get_features(qrels_test, 'datasets/CLEF/train/new_reranking_test_pred.txt', train_mode=False, max_wrote=100)\n",
    "get_features(tweets_test, 'datasets/CLEF/train/new_reranking_test_pred_TEST.txt', train_mode=False, max_wrote=100)\n",
    "\n",
    "with open('datasets/CLEF/train/new_reranking_dev_part.txt') as trainfile, \\\n",
    "    open('datasets/CLEF/train/new_reranking_test.txt') as valifile:\n",
    "    TX, Ty, Tqids, Tcom = pyltr.data.letor.read_dataset(trainfile)\n",
    "    EX, Ey, Eqids, Ecom = pyltr.data.letor.read_dataset(valifile)\n",
    "\n",
    "\n",
    "k = '169'\n",
    "TX, Ty, Tqids, VX, Vy, Vqids = TX[Tqids <= k], Ty[Tqids <= k], Tqids[Tqids <= k], TX[Tqids > k], Ty[Tqids > k], Tqids[Tqids > k]\n",
    "\n",
    "\n",
    "metric = pyltr.metrics.AP(k=5)\n",
    "\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    VX, Vy, Vqids, metric=metric, stop_after=400)\n",
    "\n",
    "metric = pyltr.metrics.AP(k=5)\n",
    "\n",
    "# Only needed if you want to perform validation (early stopping & trimming)\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    VX, Vy, Vqids, metric=metric, stop_after=400)\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.02,\n",
    "    max_features=0.3,\n",
    "    query_subsample=0.3,\n",
    "    max_leaf_nodes=12,\n",
    "    max_depth=3,\n",
    "    min_samples_leaf=1,\n",
    "    verbose=1,\n",
    "    random_state=57\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(TX, Ty, Tqids, monitor=monitor)\n",
    "\n",
    "with open('datasets/CLEF/train/new_reranking_test.txt') as valifile:\n",
    "    EX, Ey, Eqids, Ecom = pyltr.data.letor.read_dataset(valifile)\n",
    "Epred = model.predict(EX)\n",
    "metric.calc_mean(Eqids, Ey, Epred)\n",
    "\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/CLEF/train/new_reranking_test_pred.txt') as testfile:\n",
    "    EX, Ey, Eqids, Ecom = pyltr.data.letor.read_dataset(testfile)\n",
    "scores = model.predict(EX)\n",
    "q_ids = []\n",
    "doc_ids = []\n",
    "assert len(set(Ecom)) == len(Ecom)\n",
    "for el in Ecom:\n",
    "    doc_id, q_id = el.split('|')\n",
    "    q_ids.append(q_id)\n",
    "    doc_ids.append(doc_id)\n",
    "    \n",
    "ans = pd.DataFrame.from_dict({'tweet_id': q_ids, 'Q0': ['Q0'] * len(q_ids), 'vclaim_id': doc_ids,\n",
    "        'rank': ['1'] * len(q_ids),\n",
    "       'score': scores, 'tag': ['lambdamart'] * len(q_ids)})\n",
    "\n",
    "ans = ans.sort_values(['tweet_id','score'],ascending=False)\n",
    "ans.to_csv('datasets/CLEF/predictions/lambdamart_test_semi.tsv', sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python datasets/CLEF/predictions/scorer/main.py -g datasets/CLEF/predictions/gold_test.tsv -p datasets/CLEF/predictions/lambdamart_test_semi.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/CLEF/train/new_reranking_test_pred_TEST.txt') as testfile:\n",
    "    EX, Ey, Eqids, Ecom = pyltr.data.letor.read_dataset(testfile)\n",
    "scores = model.predict(EX)\n",
    "q_ids = []\n",
    "doc_ids = []\n",
    "assert len(set(Ecom)) == len(Ecom)\n",
    "for el in Ecom:\n",
    "    doc_id, q_id = el.split('|')\n",
    "    q_ids.append(q_id)\n",
    "    doc_ids.append(doc_id)\n",
    "    \n",
    "ans = pd.DataFrame.from_dict({'tweet_id': q_ids, 'Q0': ['Q0'] * len(q_ids), 'vclaim_id': doc_ids,\n",
    "        'rank': ['1'] * len(q_ids),\n",
    "       'score': scores, 'tag': ['lambdamart'] * len(q_ids)})\n",
    "\n",
    "ans = ans.sort_values(['tweet_id','score'],ascending=False)\n",
    "ans.to_csv('datasets/CLEF/predictions/lambdamart_test_semi_TEST.tsv', sep='\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python datasets/CLEF/predictions/scorer/main.py -g datasets/CLEF/predictions/qrels-test.tsv -p datasets/CLEF/predictions/lambdamart_test_semi_TEST.tsv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
